#!/usr/bin/env python2
#-*- coding: utf-8 -*-

from __future__ import print_function, division
import sys, os, os.path, itertools
pkg_dir = os.path.dirname(os.path.realpath(__file__)) + '/../../'
sys.path.append(pkg_dir)

from scipy.stats import poisson
from MPBNP import *
from MPBNP import BaseSampler, BasePredictor
from transforms import *

np.set_printoptions(suppress=True)

class Gibbs(BaseSampler):

    V_SCALE = 0
    H_SCALE = 1
    V_TRANS = 2
    H_TRANS = 3
    NUM_TRANS = 4
    
    def __init__(self, cl_mode = True, cl_device = None, record_best = True,
                 alpha = None, lam = 0.98, theta = 0.2, epislon = 0.02, init_k = 10):
        """Initialize the class.
        """
        BaseSampler.__init__(self, cl_mode = cl_mode, cl_device = cl_device, record_best = record_best)

        if cl_mode:
            program_str = open(pkg_dir + 'MPBNP/tibp/kernels/tibp_noisyor_cl.c', 'r').read()
            self.prg = cl.Program(self.ctx, program_str).build() 

        self.alpha = alpha # tendency to generate new features
        self.k = init_k    # initial number of features
        self.theta = theta # prior probability that a pixel is on in a feature image
        self.lam = lam # effecacy of a feature
        self.epislon = epislon # probability that a pixel is on by change in an actual image
        self.phi = 0.8 # prior probability that no transformation is applied
        self.samples = {'z': [], 'y': [], 'r': []} # sample storage, to be pickled

    def read_csv(self, filepath, header=True):
        """Read the data from a csv file.
        """
        BaseSampler.read_csv(self, filepath, header)
        # convert the data to the appropriate formats
        self.new_obs = []
        self.img_w, self.img_h = None, None
        for row in self.obs:
            if self.img_w is None:
                self.img_w = int(row[0])
                if self.img_w == 0 or (len(row)-1) % self.img_w != 0:
                    raise Exception('The sampler does not understand the format of the data. Did you forget to specify image width in the data file?')
            self.new_obs.append([int(_) for _ in row])
            
        self.obs = np.array(self.new_obs)[:,1:]
        if self.cl_mode:
            self.d_obs = cl.Buffer(self.ctx, self.mf.READ_ONLY | self.mf.COPY_HOST_PTR, hostbuf=self.obs.astype(np.int32))

        # self.d is the length of the flattened vectors
        self.d = self.obs.shape[1]
        self.img_h = int(self.d / self.img_w)
        self.alpha = self.N / 0.5
        return

    def direct_read_obs(self, obs):
        """Read the data from a numpy array.
        """
        BaseSampler.direct_read_obs(self, obs)
        self.d = self.obs.shape[1]
        
    def do_inference(self, init_y = None, init_z = None, init_r = None, output_file = None):
        """Perform inference on the given observations assuming data are generated by an IBP model
        with noisy-or as the likelihood function.
        @param init_y: An initial feature image matrix, where values are 0 or 1
        @param init_z: An initial feature ownership matrix, where values are 0 or 1
        """
        BaseSampler.do_inference(self, output_file=None)
        if init_y is None:
            init_y = np.random.randint(0, 2, (self.k, self.d))
        else:
            assert(type(init_y) is np.ndarray)
            assert(init_y.shape == (self.k, self.d))
        if init_z is None:
            init_z = np.random.randint(0, 2, (len(self.obs), self.k))
        else:
            assert(type(init_z) is np.ndarray)
            assert(init_z.shape == (len(self.obs), self.k))

        if init_r is None:
            init_r = np.empty(shape = (self.N, self.k, self.NUM_TRANS), dtype=np.int32)
            init_r[:,:,self.V_SCALE] = 0
            init_r[:,:,self.H_SCALE] = 0
            init_r[:,:,self.V_TRANS] = np.random.randint(0, 2, (self.N, self.k))
            init_r[:,:,self.H_TRANS] = np.random.randint(0, 2, (self.N, self.k))
        else:
            assert(init_r is None)

        if self.cl_mode:
            timing_stats = self._cl_infer_yzr(init_y, init_z, init_r)
        else:
            timing_stats = self._infer_yzr(init_y, init_z, init_r)

        # report the results
        if output_file is sys.stdout:
            if self.record_best:
                final_y, final_z, final_r = self.best_sample[0]
                num_of_feats = final_z.shape[1]
                print('parameter,value',
                      'alpha,%f' % self.alpha, 'lambda,%f' % self.lam, 'theta,%f' % self.theta,
                      'epislon,%f' % self.epislon, 'phi,%f' % self.phi, 'inferred_K,%d' % num_of_feats,
                      file = output_file, sep = '\n')
                
                np.savetxt(output_file, final_z, fmt="%d", comments='', delimiter=',',
                           header=','.join(['feature%d' % _ for _ in range(num_of_feats)]))

                for k in xrange(num_of_feats):
                    np.savetxt(output_file, final_y[k].reshape(self.img_w, self.img_h),
                               fmt="%d", delimiter=',')

                print('object', 'feature', 'v_scale', 'h_scale', 'v_translation', 'h_translation',
                      file=output_file, sep=',')
                for n in xrange(self.N):
                    for k in xrange(num_of_feats):
                        print(n, k, *final_r[n,k], file=output_file, sep=',')
        else:
            if self.record_best:
                final_y, final_z, final_r = self.best_sample[0]
                num_of_feats = final_z.shape[1]
                os.mkdir(output_file)
                print('parameter,value',
                      'alpha,%f' % self.alpha, 'lambda,%f' % self.lam, 'theta,%f' % self.theta,
                      'epislon,%f' % self.epislon, 'phi,%f' % self.phi, 'inferred_K,%d' % num_of_feats,
                      file = gzip.open(output_file + 'parameters.csv.gz', 'w'), sep = '\n')
                
                np.savetxt(gzip.open(output_file + 'feature_ownership.csv.gz', 'w'), final_z,
                           fmt="%d", comments='', delimiter=',',
                           header=','.join(['feature%d' % _ for _ in range(num_of_feats)]))

                for k in xrange(num_of_feats):
                    np.savetxt(gzip.open(output_file + 'feature_%d_image.csv.gz' % k, 'w'),
                               final_y[k].reshape(self.img_w, self.img_h), fmt="%d", delimiter=',')

                transform_fp = gzip.open(output_file + 'transformations.csv.gz', 'w')
                print('object', 'feature', 'v_scale', 'h_scale', 'v_translation', 'h_translation',
                      file = transform_fp, sep=',')
                for n in xrange(self.N):
                    for k in xrange(num_of_feats):
                        print(n, k, *final_r[n,k], file=transform_fp, sep=',')
                transform_fp.close()
            else:
                os.mkdir(output_file)
                print('parameter,value',
                      'alpha,%f' % self.alpha, 'lambda,%f' % self.lam, 'theta,%f' % self.theta,
                      'epislon,%f' % self.epislon, 'phi,%f' % self.phi, 
                      file = gzip.open(output_file + 'parameters.csv.gz', 'w'), sep = '\n')
                np.savez_compressed(output_file + 'feature_ownership.npz', self.samples['z'])
                np.savez_compressed(output_file + 'feature_images.npz', self.samples['y'])
                np.savez_compressed(output_file + 'transformations.npz', self.samples['r'])

        return timing_stats

    def _infer_yzr(self, init_y, init_z, init_r):
        """Wrapper function to start the inference on y, z and r.
        This function is not supposed to directly invoked by an end user.
        @param init_y: Passed in from do_inference()
        @param init_z: Passed in from do_inference()
        @param init_r: Passed in from do_inference()
        """
        cur_y = init_y
        cur_z = init_z
        cur_r = init_r

        a_time = time()
        if self.record_best: self.auto_save_sample(sample = (cur_y, cur_z, cur_r))
        for i in xrange(self.niter):
            temp_cur_y = self._infer_y(cur_y, cur_z, cur_r)
            temp_cur_y, temp_cur_z, temp_cur_r = self._infer_z(temp_cur_y, cur_z, cur_r)
            temp_cur_r = self._infer_r(temp_cur_y, temp_cur_z, temp_cur_r)

            if self.record_best:
                if self.auto_save_sample(sample = (temp_cur_y, temp_cur_z, temp_cur_r)):
                    cur_y, cur_z, cur_r = temp_cur_y, temp_cur_z, temp_cur_r
                if self.no_improvement(1000):
                    break                    
                
            elif i >= self.burnin:
                cur_y, cur_z, cur_r = temp_cur_y, temp_cur_z, temp_cur_r
                self.samples['z'].append(cur_z)
                self.samples['y'].append(cur_y)
                self.samples['r'].append(cur_r)

        self.total_time += time() - a_time
        return self.gpu_time, self.total_time, None

    def _infer_y(self, cur_y, cur_z, cur_r):
        """Infer feature images
        """
        # calculate the prior probability that a pixel is on
        y_on_log_prob = np.log(self.theta) * np.ones(cur_y.shape)
        y_off_log_prob = np.log(1. - self.theta) * np.ones(cur_y.shape)

        # calculate the likelihood
        on_loglik = np.empty(cur_y.shape)
        off_loglik = np.empty(cur_y.shape)
        for row in xrange(cur_y.shape[0]):
            affected_data_index = np.where(cur_z[:,row] == 1)
            for col in xrange(cur_y.shape[1]):
                old_value = cur_y[row, col]
                cur_y[row, col] = 1
                on_loglik[row, col] = self._loglik_nth(cur_y, cur_z, cur_r, n = affected_data_index)
                cur_y[row, col] = 0
                off_loglik[row, col] = self._loglik_nth(cur_y, cur_z, cur_r, n = affected_data_index)
                cur_y[row, col] = old_value

        # add to the prior
        y_on_log_prob += on_loglik
        y_off_log_prob += off_loglik

        ew_max = np.maximum(y_on_log_prob, y_off_log_prob)
        y_on_log_prob -= ew_max
        y_off_log_prob -= ew_max
        
        # normalize
        y_on_prob = np.exp(y_on_log_prob) / (np.exp(y_on_log_prob) + np.exp(y_off_log_prob))
        cur_y = np.random.binomial(1, y_on_prob)

        return cur_y

    def _infer_z(self, cur_y, cur_z, cur_r):
        """Infer feature ownership
        """
        N = float(len(self.obs))
        z_col_sum = cur_z.sum(axis = 0)

        # calculate the IBP prior on feature ownership for existing features
        m_minus = z_col_sum - cur_z
        on_prob = m_minus / N
        off_prob = 1 - m_minus / N
        
        # add loglikelihood of data
        for row in xrange(cur_z.shape[0]):
            for col in xrange(cur_z.shape[1]):
                old_value = cur_z[row, col]
                cur_z[row, col] = 1
                on_prob[row, col] = on_prob[row, col] * np.exp(self._loglik_nth(cur_y, cur_z, cur_r, n = row))
                cur_z[row, col] = 0
                off_prob[row, col] = off_prob[row, col] * np.exp(self._loglik_nth(cur_y, cur_z, cur_r, n = row))
                cur_z[row, col] = old_value

        # normalize the probability
        on_prob = on_prob / (on_prob + off_prob)

        # sample the values
        cur_z = np.random.binomial(1, on_prob)

        # sample new features use importance sampling
        k_new = self._sample_k_new(cur_y, cur_z, cur_r)
        if k_new:
            cur_y, cur_z, cur_r = k_new
        
        # delete empty feature images
        non_empty_feat_img = np.where(cur_y.sum(axis = 1) > 0)
        cur_y = cur_y[non_empty_feat_img[0],:]
        cur_z = cur_z[:,non_empty_feat_img[0]]
        cur_r = np.array([_[non_empty_feat_img[0],:] for _ in cur_r])
        
        # delete null features
        active_feat_col = np.where(cur_z.sum(axis = 0) > 0)
        cur_z = cur_z[:,active_feat_col[0]]
        cur_y = cur_y[active_feat_col[0],:]
        cur_r = np.array([_[active_feat_col[0],:] for _ in cur_r])

        # update self.k
        self.k = cur_z.shape[1]
        
        return cur_y, cur_z, cur_r

    def _infer_r(self, cur_y, cur_z, cur_r):
        """Infer transformations.
        """
        rand_v = np.random.randint(0, self.img_h, size=(cur_z.shape[0], cur_z.shape[1]))
        rand_h = np.random.randint(0, self.img_w, size=(cur_z.shape[0], cur_z.shape[1]))
        rand_v_scale = np.random.randint(-self.img_h+2, self.img_h, size=(cur_z.shape[0], cur_z.shape[1]))
        rand_h_scale = np.random.randint(-self.img_w+2, self.img_w, size=(cur_z.shape[0], cur_z.shape[1]))
        # iterate over each transformation and resample it 
        for nth_img in xrange(cur_r.shape[0]):
            for kth_feature in xrange(cur_r.shape[1]):
                old_loglik = self._loglik_nth(cur_y, cur_z, cur_r, n=nth_img)

                # resample vertical translation
                old_v_trans = cur_r[nth_img, kth_feature, self.V_TRANS]
                # set a new vertical transformation
                cur_r[nth_img, kth_feature, self.V_TRANS] = rand_v[nth_img, kth_feature] #np.random.randint(0, self.img_h)

                old_logprior = np.log(abs((old_v_trans > 0) - self.phi))
                new_logprior = np.log(abs((rand_v[nth_img, kth_feature] > 0) - self.phi))
                
                new_loglik = self._loglik_nth(cur_y, cur_z, cur_r, n = nth_img)
                move_prob = 1 / (1 + np.exp(old_loglik + old_logprior - new_loglik - new_logprior))
                if random.random() > move_prob: # revert changes if move_prob too small
                    cur_r[nth_img, kth_feature, self.V_TRANS] = old_v_trans
                else:
                    old_loglik = new_loglik

                # resample horizontal translation
                old_h_trans = cur_r[nth_img, kth_feature, self.H_TRANS]
                # set a new vertical transformation
                cur_r[nth_img, kth_feature, self.H_TRANS] = rand_h[nth_img, kth_feature]

                old_logprior = np.log(abs((old_h_trans > 0) - self.phi))
                new_logprior = np.log(abs((rand_h[nth_img, kth_feature] > 0) - self.phi))

                new_loglik = self._loglik_nth(cur_y, cur_z, cur_r, n = nth_img)
                move_prob = 1 / (1 + np.exp(old_loglik + old_logprior - new_loglik - new_logprior))
                if random.random() > move_prob: # revert changes if move_prob too small
                    cur_r[nth_img, kth_feature, self.H_TRANS] = old_h_trans
                else:
                    old_loglik = new_loglik

                # resample scale percentage
                old_v_scale = cur_r[nth_img, kth_feature, self.V_SCALE]
                # set a new vertical scale
                cur_r[nth_img, kth_feature, self.V_SCALE] = rand_v_scale[nth_img, kth_feature]

                old_logprior = np.log(abs((old_v_scale > 0) - self.phi))
                new_logprior = np.log(abs((rand_v_scale[nth_img, kth_feature] > 0) - self.phi))

                new_loglik = self._loglik_nth(cur_y, cur_z, cur_r, n = nth_img)
                move_prob = 1 / (1 + np.exp(old_loglik + old_logprior - new_loglik - new_logprior))
                if random.random() > move_prob: # revert changes if move_prob too small
                    cur_r[nth_img, kth_feature, self.V_SCALE] = old_v_scale
                else:
                    old_loglik = new_loglik

                # resample scale percentage
                old_h_scale = cur_r[nth_img, kth_feature, self.H_SCALE]
                # set a new horizontal scale
                cur_r[nth_img, kth_feature, self.H_SCALE] = rand_h_scale[nth_img, kth_feature]

                old_logprior = np.log(abs((old_h_scale > 0) - self.phi))
                new_logprior = np.log(abs((rand_h_scale[nth_img, kth_feature] > 0) - self.phi))

                new_loglik = self._loglik_nth(cur_y, cur_z, cur_r, n = nth_img)
                move_prob = 1 / (1 + np.exp(old_loglik + old_logprior - new_loglik - new_logprior))
                if random.random() > move_prob: # revert changes if move_prob too small
                    cur_r[nth_img, kth_feature, self.H_SCALE] = old_h_scale
                    
        return cur_r
    
    def _sample_k_new(self, cur_y, cur_z, cur_r):
        """Sample new features for all rows using Metropolis hastings.
        (This is a heuristic strategy aiming for easy parallelization in an 
        equivalent GPU implementation. We here have effectively treated the
        current Z as a snapshot frozen in time, and each new k is based on
        this frozen snapshot of Z. In a more correct procedure, we should
        go through the rows and sample k new for each row given all previously
        sampled new ks.)
        """
        N = float(len(self.obs))
        #old_loglik = self._loglik(cur_y, cur_z, cur_r)

        k_new_count = np.random.poisson(self.alpha / N)
        if k_new_count == 0: return False
            
        # modify the feature ownership matrix
        cur_z_new = np.hstack((cur_z, np.random.randint(0, 2, size = (cur_z.shape[0], k_new_count))))
        #cur_z_new[:, [xrange(-k_new_count,0)]] = 1
        # propose feature images by sampling from the prior distribution
        cur_y_new = np.vstack((cur_y, np.random.binomial(1, self.theta, (k_new_count, self.d))))
        cur_r_new = np.array([np.vstack((_, np.zeros((k_new_count, self.NUM_TRANS)))) for _ in cur_r])
        return cur_y_new.astype(np.int32), cur_z_new.astype(np.int32), cur_r_new.astype(np.int32)

    def _sample_lam(self, cur_y, cur_z):
        """Resample the value of lambda.
        """
        old_loglik = self._loglik(cur_y, cur_z)
        old_lam = self.lam
    
        # modify the feature ownership matrix
        self.lam = np.random.beta(1,1)
        new_loglik = self._loglik(cur_y, cur_z)
        move_prob = 1 / (1 + np.exp(old_loglik - new_loglik))
        if random.random() < move_prob:
            pass
        else:
            self.lam = old_lam

    def _sample_epislon(self, cur_y, cur_z):
        """Resample the value of epislon
        """
        old_loglik = self._loglik(cur_y, cur_z)
        old_epislon = self.epislon
    
        # modify the feature ownership matrix
        self.epislon = np.random.beta(1,1)
        new_loglik = self._loglik(cur_y, cur_z)
        move_prob = 1 / (1 + np.exp(old_loglik - new_loglik));
        if random.random() < move_prob:
            pass
        else:
            self.epislon = old_epislon

    def _loglik_nth(self, cur_y, cur_z, cur_r, n):
        """Calculate the loglikelihood of the nth data point
        given Y, Z and R.
        """
        assert(cur_z.shape[1] == cur_y.shape[0] == cur_r.shape[1])

        if type(n) is int: n = [n]
        else: n = n[0]
        not_on_p = np.empty((len(n), cur_y.shape[1]))
        
        # transform the feature images to obtain the effective y
        # this needs to be done on a per object basis

        for i in xrange(len(n)):
            nth = n[i]
            nth_y = copy.deepcopy(cur_y) # the transformed cur_y with respect to nth
            kth_feat = 0
            for r_feat in cur_r[nth]: # r_feat refers to the transforms applied one feature
                nth_y[kth_feat] = scale_manual(nth_y[kth_feat], self.img_w, r_feat[self.H_SCALE], r_feat[self.V_SCALE])
                nth_y[kth_feat] = v_translate(nth_y[kth_feat], self.img_w, r_feat[self.V_TRANS])
                nth_y[kth_feat] = h_translate(nth_y[kth_feat], self.img_w, r_feat[self.H_TRANS])
                kth_feat += 1
                
            not_on_p[i] = np.power(1. - self.lam, np.dot(cur_z[nth], nth_y)) * (1. - self.epislon)
        loglik = np.log(np.abs(self.obs[n] - not_on_p)).sum()
        return loglik

    def _loglik(self, cur_y, cur_z, cur_r):
        """Calculate the loglikelihood of data given Y, Z and R.
        """
        assert(cur_z.shape[1] == cur_y.shape[0] == cur_r.shape[1])

        not_on_p = np.empty((self.N, self.d))

        # transform the feature images to obtain the effective y
        # this needs to be done on a per object basis
        
        for nth in xrange(self.N):
            nth_y = copy.deepcopy(cur_y) # the transformed cur_y with respect to nth
            kth_feat = 0
            for r_feat in cur_r[nth]: # r_feat refers to the transforms applied one feature
                nth_y[kth_feat] = scale_manual(nth_y[kth_feat], self.img_w, r_feat[self.H_SCALE], r_feat[self.V_SCALE])
                nth_y[kth_feat] = v_translate(nth_y[kth_feat], self.img_w, r_feat[self.V_TRANS])
                nth_y[kth_feat] = h_translate(nth_y[kth_feat], self.img_w, r_feat[self.H_TRANS])
                kth_feat += 1
                
            not_on_p[nth] = np.power(1. - self.lam, np.dot(cur_z[nth], nth_y)) * (1. - self.epislon)
        
        loglik_mat = np.log(np.abs(self.obs - not_on_p))
        return loglik_mat.sum()

    def _z_by_ry(self, cur_y, cur_z, cur_r):
        """
        """
        z_by_ry = np.empty(shape = (cur_z.shape[0], cur_y.shape[1]), dtype=np.int64)
        for nth in xrange(self.N):
            nth_y = copy.deepcopy(cur_y) # the transformed cur_y with respect to nth
            kth_feat = 0
            for r_feat in cur_r[nth]: # r_feat refers to the transforms applied one feature
                nth_y[kth_feat] = v_translate(nth_y[kth_feat], self.img_w, r_feat[self.V_TRANS])
                nth_y[kth_feat] = h_translate(nth_y[kth_feat], self.img_w, r_feat[self.H_TRANS])
                kth_feat += 1
            z_by_ry[nth,] = np.dot(cur_z[nth], nth_y)
        return z_by_ry

    def _cl_infer_yzr(self, init_y, init_z, init_r):
        """Wrapper function to start the inference on y and z.
        This function is not supposed to directly invoked by an end user.
        @param init_y: Passed in from do_inference()
        @param init_z: Passed in from do_inference()
        @param init_r: Passed in from do_inference()
        """
        total_time = time()
        cur_y = init_y.astype(np.int32)
        cur_z = init_z.astype(np.int32)
        cur_r = init_r.astype(np.int32) # this is fine with only translations

        if self.record_best: self.auto_save_sample(sample = (cur_y, cur_z, cur_r))
        for i in xrange(self.niter):
            a_time = time()
            d_cur_z = cl.Buffer(self.ctx, self.mf.READ_WRITE | self.mf.COPY_HOST_PTR, hostbuf = cur_z.astype(np.int32))
            d_cur_y = cl.Buffer(self.ctx, self.mf.READ_WRITE | self.mf.COPY_HOST_PTR, hostbuf = cur_y.astype(np.int32))
            d_cur_r = cl.Buffer(self.ctx, self.mf.READ_WRITE | self.mf.COPY_HOST_PTR, hostbuf = cur_r.astype(np.int32))
            self.gpu_time += time() - a_time

            d_cur_y = self._cl_infer_y(cur_y, cur_z, cur_r, d_cur_y, d_cur_z, d_cur_r)
            d_cur_z = self._cl_infer_z(cur_y, cur_z, cur_r, d_cur_y, d_cur_z, d_cur_r)
            temp_cur_r = self._cl_infer_r(cur_y, cur_z, cur_r, d_cur_y, d_cur_z, d_cur_r)

            a_time = time()
            temp_cur_y = np.empty_like(cur_y)
            cl.enqueue_copy(self.queue, temp_cur_y, d_cur_y)
            temp_cur_z = np.empty_like(cur_z)
            cl.enqueue_copy(self.queue, temp_cur_z, d_cur_z)
            self.gpu_time += time() - a_time
            
            temp_cur_y, temp_cur_z, temp_cur_r = self._cl_infer_k_new(temp_cur_y, temp_cur_z, temp_cur_r)

            if self.record_best:
                if self.auto_save_sample(sample = (temp_cur_y, temp_cur_z, temp_cur_r)):
                    print('Number of features:', cur_z.shape[1], file=sys.stderr)
                    cur_y, cur_z, cur_r = temp_cur_y, temp_cur_z, temp_cur_r
                if self.no_improvement(1000):
                    break                    
            elif i >= self.burnin:
                cur_y, cur_z, cur_r = temp_cur_y, temp_cur_z, temp_cur_r
                self.samples['z'].append(cur_z)
                self.samples['y'].append(cur_y)
                self.samples['y'].append(cur_r)
            
        self.total_time += time() - total_time

        return self.gpu_time, self.total_time, None

    def _cl_infer_y(self, cur_y, cur_z, cur_r, d_cur_y, d_cur_z, d_cur_r):
        """Infer feature images
        """
        a_time = time()
        d_z_by_ry = cl.Buffer(self.ctx, self.mf.READ_WRITE | self.mf.COPY_HOST_PTR, 
                              hostbuf = np.empty(shape = self.obs.shape, dtype = np.int32))
        d_rand = cl.Buffer(self.ctx, self.mf.READ_ONLY | self.mf.COPY_HOST_PTR, 
                           hostbuf = np.random.random(cur_y.shape).astype(np.float32))
        transformed_y = np.empty(shape = (self.obs.shape[0], cur_z.shape[1], self.obs.shape[1]), dtype = np.int32)
        d_transformed_y = cl.Buffer(self.ctx, self.mf.READ_WRITE | self.mf.COPY_HOST_PTR, hostbuf = transformed_y)
        d_temp_y = cl.Buffer(self.ctx, self.mf.READ_WRITE | self.mf.COPY_HOST_PTR, hostbuf = transformed_y)

        # first transform the feature images and calculate z_by_ry
        self.prg.compute_z_by_ry(self.queue, cur_z.shape, (1, cur_z.shape[1]), 
                                 d_cur_y, d_cur_z, d_cur_r, d_transformed_y, d_temp_y, d_z_by_ry,
                                 np.int32(self.obs.shape[0]), np.int32(self.obs.shape[1]), np.int32(cur_y.shape[0]),
                                 np.int32(self.img_w))

        # calculate the prior probability that a pixel is on
        self.prg.sample_y(self.queue, cur_y.shape, None,
                          d_cur_y, d_cur_z, d_z_by_ry, d_cur_r, self.d_obs, d_rand, 
                          np.int32(self.N), np.int32(self.d), np.int32(cur_y.shape[0]), np.int32(self.img_w),
                          np.float32(self.lam), np.float32(self.epislon), np.float32(self.theta))

        self.gpu_time += time() - a_time
        return d_cur_y

    def _cl_infer_z(self, cur_y, cur_z, cur_r, d_cur_y, d_cur_z, d_cur_r):
        """Infer feature ownership
        """
        a_time = time()
        d_z_by_ry = cl.Buffer(self.ctx, self.mf.READ_WRITE | self.mf.COPY_HOST_PTR, 
                              hostbuf = np.empty(shape = self.obs.shape, dtype = np.int32))
        d_z_col_sum = cl.Buffer(self.ctx, self.mf.READ_ONLY | self.mf.COPY_HOST_PTR, 
                                hostbuf = cur_z.sum(axis = 0).astype(np.int32))
        d_rand = cl.Buffer(self.ctx, self.mf.READ_ONLY | self.mf.COPY_HOST_PTR, 
                           hostbuf = np.random.random(cur_z.shape).astype(np.float32))
        transformed_y = np.empty(shape = (self.obs.shape[0], cur_z.shape[1], self.obs.shape[1]), dtype = np.int32)
        d_transformed_y = cl.Buffer(self.ctx, self.mf.READ_WRITE | self.mf.COPY_HOST_PTR, hostbuf = transformed_y)
        d_temp_y = cl.Buffer(self.ctx, self.mf.READ_WRITE | self.mf.COPY_HOST_PTR, hostbuf = transformed_y)


        # first transform the feature images and calculate z_by_ry
        self.prg.compute_z_by_ry(self.queue, cur_z.shape, (1, cur_z.shape[1]), 
                                 d_cur_y, d_cur_z, d_cur_r, d_transformed_y, d_temp_y, d_z_by_ry,
                                 np.int32(self.obs.shape[0]), np.int32(self.obs.shape[1]), np.int32(cur_y.shape[0]),
                                 np.int32(self.img_w))

        # calculate the prior probability that a pixel is on
        self.prg.sample_z(self.queue, cur_z.shape, None,
                          d_cur_y, d_cur_z, d_cur_r, d_z_by_ry, d_z_col_sum, self.d_obs, d_rand, 
                          np.int32(self.N), np.int32(self.d), np.int32(cur_y.shape[0]), np.int32(self.img_w),
                          np.float32(self.lam), np.float32(self.epislon), np.float32(self.theta))

        self.gpu_time += time() - a_time
        return d_cur_z
        
    def _cl_infer_k_new(self, cur_y, cur_z, cur_r):

        # sample new features use importance sampling
        k_new = self._sample_k_new(cur_y, cur_z, cur_r)
        if k_new:
            cur_y, cur_z, cur_r = k_new

        # delete empty feature images
        non_empty_feat_img = np.where(cur_y.sum(axis = 1) > 0)
        cur_y = cur_y[non_empty_feat_img[0],:].astype(np.int32)
        cur_z = cur_z[:,non_empty_feat_img[0]].astype(np.int32)
        cur_r = np.array([_[non_empty_feat_img[0],:] for _ in cur_r]).astype(np.int32)
        
        # delete null features
        active_feat_col = np.where(cur_z.sum(axis = 0) > 0)
        cur_z = cur_z[:,active_feat_col[0]].astype(np.int32)
        cur_y = cur_y[active_feat_col[0],:].astype(np.int32)
        cur_r = np.array([_[active_feat_col[0],:] for _ in cur_r]).astype(np.int32)

        # update self.k
        self.k = cur_z.shape[1]

        z_s0, z_s1 = cur_z.shape
        cur_z = cur_z.reshape((z_s0 * z_s1, 1))
        cur_z = cur_z.reshape((z_s0, z_s1))

        y_s0, y_s1 = cur_y.shape
        cur_y = cur_y.reshape((y_s0 * y_s1, 1))
        cur_y = cur_y.reshape((y_s0, y_s1))

        r_s0, r_s1, r_s2 = cur_r.shape
        cur_r = cur_r.reshape((r_s0 * r_s1 * r_s2, 1))
        cur_r = cur_r.reshape((r_s0, r_s1, r_s2))

        return cur_y, cur_z, cur_r

    def _cl_infer_r(self, cur_y, cur_z, cur_r, d_cur_y, d_cur_z, d_cur_r):
        """Infer transformations using opencl.
        Note: the algorithm works because resampling one value of cur_r at one time
        only affects the loglikelihood of the corresponding image. Therefore, it is
        possible to resample one aspect of transformation for all images at the same
        time, as long as the new values are accepted / rejected independently of
        each other.
        """
        a_time = time()
        d_z_by_ry_old = cl.array.empty(self.queue, self.obs.shape, np.int32, allocator=self.mem_pool)
        d_z_by_ry_new = cl.array.empty(self.queue, self.obs.shape, np.int32, allocator=self.mem_pool)
        d_replace_r = cl.array.empty(self.queue, (self.N,), np.int32, allocator=self.mem_pool)
        d_rand = cl.Buffer(self.ctx, self.mf.READ_ONLY | self.mf.COPY_HOST_PTR,
                           hostbuf=np.random.random(self.N).astype(np.float32))
        transformed_y = np.empty(shape = (self.obs.shape[0], cur_z.shape[1], self.obs.shape[1]), dtype = np.int32)
        d_transformed_y = cl.Buffer(self.ctx, self.mf.READ_WRITE | self.mf.COPY_HOST_PTR, hostbuf = transformed_y)
        d_temp_y = cl.Buffer(self.ctx, self.mf.READ_WRITE | self.mf.COPY_HOST_PTR, hostbuf = transformed_y)

        ########### Dealing with vertical translations first ##########
        d_cur_r = cl.Buffer(self.ctx, self.mf.READ_ONLY | self.mf.COPY_HOST_PTR, hostbuf = cur_r.astype(np.int32))

        # calculate the z_by_ry_old under old transformations
        self.prg.compute_z_by_ry(self.queue, cur_z.shape, (1, cur_z.shape[1]),
                                 d_cur_y, d_cur_z, d_cur_r, d_transformed_y, d_temp_y, d_z_by_ry_old.data, 
                                 np.int32(self.obs.shape[0]), np.int32(self.obs.shape[1]), np.int32(cur_y.shape[0]),
                                 np.int32(self.img_w))

        # calculate the z_by_ry_new under new randomly generated transformations
        cur_r_new = np.copy(cur_r)
        cur_r_new[:,:,self.V_TRANS] = np.random.randint(0, self.img_h, size = (cur_r_new.shape[0], cur_r_new.shape[1]))
        d_cur_r_new = cl.Buffer(self.ctx, self.mf.READ_ONLY | self.mf.COPY_HOST_PTR, hostbuf = cur_r_new.astype(np.int32))
        
        self.prg.compute_z_by_ry(self.queue, cur_z.shape, (1, cur_z.shape[1]),
                                 d_cur_y, d_cur_z, d_cur_r_new, d_transformed_y, d_temp_y, d_z_by_ry_new.data, 
                                 np.int32(self.obs.shape[0]), np.int32(self.obs.shape[1]), np.int32(cur_y.shape[0]),
                                 np.int32(self.img_w))

        # reject or accept newly proposed transformations on a per-object basis
        d_logprior_old = cl.Buffer(self.ctx, self.mf.READ_ONLY | self.mf.COPY_HOST_PTR, 
                                   hostbuf = np.log(abs((cur_r[:,:,self.V_TRANS] > 0) - self.phi)).astype(np.float32)) 
        d_logprior_new = cl.Buffer(self.ctx, self.mf.READ_ONLY | self.mf.COPY_HOST_PTR, 
                                   hostbuf = np.log(abs((cur_r_new[:,:,self.V_TRANS] > 0) - self.phi)).astype(np.float32)) 

        self.prg.sample_r(self.queue, (self.N, ), None,
                          d_replace_r.data, d_z_by_ry_old.data, d_z_by_ry_new.data, 
                          d_logprior_old, d_logprior_new, self.d_obs, d_rand,
                          np.int32(self.obs.shape[0]), np.int32(self.obs.shape[1]), np.int32(cur_y.shape[0]),
                          np.float32(self.lam), np.float32(self.epislon))

        replace_r = d_replace_r.get()
        cur_r[np.where(replace_r == 1)] = cur_r_new[np.where(replace_r == 1)]

        ########### Dealing with horizontal translations next ##########
        d_cur_r = cl.Buffer(self.ctx, self.mf.READ_ONLY | self.mf.COPY_HOST_PTR, hostbuf = cur_r.astype(np.int32))

        # calculate the z_by_ry_old under old transformations
        self.prg.compute_z_by_ry(self.queue, cur_z.shape, (1, cur_z.shape[1]),
                                 d_cur_y, d_cur_z, d_cur_r, d_transformed_y, d_temp_y, d_z_by_ry_old.data, 
                                 np.int32(self.obs.shape[0]), np.int32(self.obs.shape[1]), np.int32(cur_y.shape[0]),
                                 np.int32(self.img_w))

        # calculate the z_by_ry_new under new randomly generated transformations
        cur_r_new = np.copy(cur_r)
        cur_r_new[:,:,self.H_TRANS] = np.random.randint(0, self.img_w, size = (cur_r_new.shape[0], cur_r_new.shape[1]))
        d_cur_r_new = cl.Buffer(self.ctx, self.mf.READ_ONLY | self.mf.COPY_HOST_PTR, hostbuf = cur_r_new.astype(np.int32))
        
        self.prg.compute_z_by_ry(self.queue, cur_z.shape, (1, cur_z.shape[1]),
                                 d_cur_y, d_cur_z, d_cur_r_new, d_transformed_y, d_temp_y, d_z_by_ry_new.data, 
                                 np.int32(self.obs.shape[0]), np.int32(self.obs.shape[1]), np.int32(cur_y.shape[0]),
                                 np.int32(self.img_w))

        # reject or accept newly proposed transformations on a per-object basis
        d_logprior_old = cl.Buffer(self.ctx, self.mf.READ_ONLY | self.mf.COPY_HOST_PTR, 
                                   hostbuf = np.log(abs((cur_r[:,:,self.H_TRANS] > 0) - self.phi)).astype(np.float32)) 
        d_logprior_new = cl.Buffer(self.ctx, self.mf.READ_ONLY | self.mf.COPY_HOST_PTR, 
                                   hostbuf = np.log(abs((cur_r_new[:,:,self.H_TRANS] > 0) - self.phi)).astype(np.float32)) 

        self.prg.sample_r(self.queue, (self.N, ), None,
                          d_replace_r.data, d_z_by_ry_old.data, d_z_by_ry_new.data, 
                          d_logprior_old, d_logprior_new, self.d_obs, d_rand,
                          np.int32(self.obs.shape[0]), np.int32(self.obs.shape[1]), np.int32(cur_y.shape[0]),
                          np.float32(self.lam), np.float32(self.epislon))

        replace_r = d_replace_r.get()
        cur_r[np.where(replace_r == 1)] = cur_r_new[np.where(replace_r == 1)]

        ########### Dealing with vertical scaling next ##########
        d_cur_r = cl.Buffer(self.ctx, self.mf.READ_ONLY | self.mf.COPY_HOST_PTR, hostbuf = cur_r.astype(np.int32))

        # calculate the z_by_ry_old under old transformations
        self.prg.compute_z_by_ry(self.queue, cur_z.shape, (1, cur_z.shape[1]),
                                 d_cur_y, d_cur_z, d_cur_r_new, d_transformed_y, d_temp_y, d_z_by_ry_new.data, 
                                 np.int32(self.obs.shape[0]), np.int32(self.obs.shape[1]), np.int32(cur_y.shape[0]),
                                 np.int32(self.img_w))

        # calculate the z_by_ry_new under new randomly generated transformations
        cur_r_new = np.copy(cur_r)
        cur_r_new[:,:,self.V_SCALE] = np.random.randint(-self.img_h+2, self.img_h, size = (cur_r_new.shape[0], cur_r_new.shape[1]))
        d_cur_r_new = cl.Buffer(self.ctx, self.mf.READ_ONLY | self.mf.COPY_HOST_PTR, hostbuf = cur_r_new.astype(np.int32))
        
        self.prg.compute_z_by_ry(self.queue, cur_z.shape, (1, cur_z.shape[1]),
                                 d_cur_y, d_cur_z, d_cur_r_new, d_transformed_y, d_temp_y, d_z_by_ry_new.data, 
                                 np.int32(self.obs.shape[0]), np.int32(self.obs.shape[1]), np.int32(cur_y.shape[0]),
                                 np.int32(self.img_w))

        # reject or accept newly proposed transformations on a per-object basis
        d_logprior_old = cl.Buffer(self.ctx, self.mf.READ_ONLY | self.mf.COPY_HOST_PTR, 
                                   hostbuf = np.log(abs((cur_r[:,:,self.V_SCALE] > 0) - self.phi)).astype(np.float32)) 
        d_logprior_new = cl.Buffer(self.ctx, self.mf.READ_ONLY | self.mf.COPY_HOST_PTR, 
                                   hostbuf = np.log(abs((cur_r_new[:,:,self.V_SCALE] > 0) - self.phi)).astype(np.float32)) 

        self.prg.sample_r(self.queue, (self.N, ), None,
                          d_replace_r.data, d_z_by_ry_old.data, d_z_by_ry_new.data, 
                          d_logprior_old, d_logprior_new, self.d_obs, d_rand,
                          np.int32(self.obs.shape[0]), np.int32(self.obs.shape[1]), np.int32(cur_y.shape[0]),
                          np.float32(self.lam), np.float32(self.epislon))

        replace_r = d_replace_r.get()
        cur_r[np.where(replace_r == 1)] = cur_r_new[np.where(replace_r == 1)]


        ########### Dealing with horizontal scaling next ##########
        d_cur_r = cl.Buffer(self.ctx, self.mf.READ_ONLY | self.mf.COPY_HOST_PTR, hostbuf = cur_r.astype(np.int32))

        # calculate the z_by_ry_old under old transformations
        self.prg.compute_z_by_ry(self.queue, cur_z.shape, (1, cur_z.shape[1]),
                                 d_cur_y, d_cur_z, d_cur_r_new, d_transformed_y, d_temp_y, d_z_by_ry_new.data, 
                                 np.int32(self.obs.shape[0]), np.int32(self.obs.shape[1]), np.int32(cur_y.shape[0]),
                                 np.int32(self.img_w))

        # calculate the z_by_ry_new under new randomly generated transformations
        cur_r_new = np.copy(cur_r)
        cur_r_new[:,:,self.H_SCALE] = np.random.randint(-self.img_w+2, self.img_w, size = (cur_r_new.shape[0], cur_r_new.shape[1]))
        d_cur_r_new = cl.Buffer(self.ctx, self.mf.READ_ONLY | self.mf.COPY_HOST_PTR, hostbuf = cur_r_new.astype(np.int32))
        
        self.prg.compute_z_by_ry(self.queue, cur_z.shape, (1, cur_z.shape[1]),
                                 d_cur_y, d_cur_z, d_cur_r_new, d_transformed_y, d_temp_y, d_z_by_ry_new.data, 
                                 np.int32(self.obs.shape[0]), np.int32(self.obs.shape[1]), np.int32(cur_y.shape[0]),
                                 np.int32(self.img_w))

        # reject or accept newly proposed transformations on a per-object basis
        d_logprior_old = cl.Buffer(self.ctx, self.mf.READ_ONLY | self.mf.COPY_HOST_PTR, 
                                   hostbuf = np.log(abs((cur_r[:,:,self.H_SCALE] > 0) - self.phi)).astype(np.float32)) 
        d_logprior_new = cl.Buffer(self.ctx, self.mf.READ_ONLY | self.mf.COPY_HOST_PTR, 
                                   hostbuf = np.log(abs((cur_r_new[:,:,self.H_SCALE] > 0) - self.phi)).astype(np.float32)) 
        self.prg.sample_r(self.queue, (self.N, ), None,
                          d_replace_r.data, d_z_by_ry_old.data, d_z_by_ry_new.data, 
                          d_logprior_old, d_logprior_new, self.d_obs, d_rand,
                          np.int32(self.obs.shape[0]), np.int32(self.obs.shape[1]), np.int32(cur_y.shape[0]),
                          np.float32(self.lam), np.float32(self.epislon))

        replace_r = d_replace_r.get()
        cur_r[np.where(replace_r == 1)] = cur_r_new[np.where(replace_r == 1)]

        self.gpu_time += time() - a_time
        return cur_r

    
    def _logprob(self, sample):
        """Calculate the joint log probability of data and model given a sample.
        """
        cur_y, cur_z, cur_r = sample
        log_prior = 0
        log_lik = 0
        if cur_z.shape[1] == 0: return -999999999.9
    
        if self.cl_mode:
            a_time = time()
            d_cur_z = cl.Buffer(self.ctx, self.mf.READ_ONLY | self.mf.COPY_HOST_PTR, hostbuf = cur_z.astype(np.int32))
            d_cur_y = cl.Buffer(self.ctx, self.mf.READ_ONLY | self.mf.COPY_HOST_PTR, hostbuf = cur_y.astype(np.int32))
            d_cur_r = cl.Buffer(self.ctx, self.mf.READ_ONLY | self.mf.COPY_HOST_PTR, hostbuf = cur_r.astype(np.int32))
            d_z_by_ry = cl.Buffer(self.ctx, self.mf.READ_WRITE | self.mf.COPY_HOST_PTR, 
                                  hostbuf = np.empty(shape = self.obs.shape, dtype = np.int32))
            transformed_y = np.empty(shape = (self.obs.shape[0], cur_z.shape[1], self.obs.shape[1]), dtype = np.int32)
            d_transformed_y = cl.Buffer(self.ctx, self.mf.READ_WRITE | self.mf.COPY_HOST_PTR, 
                                        hostbuf = transformed_y)
            d_temp_y = cl.Buffer(self.ctx, self.mf.READ_WRITE | self.mf.COPY_HOST_PTR, 
                                 hostbuf = transformed_y)
            
            # calculate the log prior of Z
            d_logprior_z = cl.array.empty(self.queue, cur_z.shape, np.float32)
            self.prg.logprior_z(self.queue, cur_z.shape, (1, cur_z.shape[1]), 
                                d_cur_z, d_logprior_z.data, 
                                cl.LocalMemory(cur_z[0].nbytes), #cl.LocalMemory(cur_z.nbytes),
                                np.int32(self.N), np.int32(cur_y.shape[1]), np.int32(cur_z.shape[1]), 
                                np.float32(self.alpha))

            # calculate the loglikelihood of data
            # first transform the feature images and calculate z_by_ry
            self.prg.compute_z_by_ry(self.queue, cur_z.shape, (1, cur_z.shape[1]), 
                                     d_cur_y, d_cur_z, d_cur_r, d_transformed_y, d_temp_y, d_z_by_ry,
                                     np.int32(self.obs.shape[0]), np.int32(self.obs.shape[1]), np.int32(cur_y.shape[0]),
                                     np.int32(self.img_w))
            
            loglik = np.empty(shape = self.obs.shape, dtype = np.float32)
            d_loglik = cl.Buffer(self.ctx, self.mf.READ_WRITE | self.mf.COPY_HOST_PTR, hostbuf = loglik)
            self.prg.loglik(self.queue, self.obs.shape, None, 
                            d_z_by_ry, self.d_obs, d_loglik,
                            np.int32(self.N), np.int32(cur_y.shape[1]), np.int32(cur_z.shape[1]), 
                            np.float32(self.lam), np.float32(self.epislon))
            
            cl.enqueue_copy(self.queue, loglik, d_loglik)
            log_lik = loglik.sum()
            self.gpu_time += time() - a_time

            # calculate the prior probability of Y
            num_on, num_off = (cur_y == 1).sum(), (cur_y == 0).sum()
            log_prior = num_on * np.log(self.theta) + num_off * np.log(1 - self.theta) + d_logprior_z.get().sum()

            # calculate the prior probability of R
            # we implement a slight bias towards no transformation
            log_prior += (cur_r > 0).sum() * np.log(1 - self.phi) + (cur_r == 0).sum() * np.log(self.phi)
            
        else:
            # calculate the prior probability of Z
            feat_count = cur_z.cumsum(axis = 0)
            for n in xrange(cur_z.shape[0]):
                num_novel = 0
                for k in xrange(cur_z.shape[1]):
                    m = feat_count[n,k] - cur_z[n,k]#cur_z[:n,k].sum()
                    if m > 0:
                        if cur_z[n,k] == 1: log_prior += np.log(m / (n+1.0))
                        else: log_prior += np.log(1 - m / (n + 1.0))
                    else: 
                        if cur_z[n,k] == 1: num_novel += 1
                if num_novel > 0:
                    log_prior += poisson.logpmf(num_novel, self.alpha / (n+1.0))

            # calculate the prior probability of Y
            num_on = (cur_y == 1).sum()
            num_off = (cur_y == 0).sum()
            log_prior += num_on * np.log(self.theta) + num_off * np.log(1 - self.theta)

            # calculate the prior probability of R
            # we implement a slight bias towards no transformation
            log_prior += (cur_r > 0).sum() * np.log(1 - self.phi) + (cur_r == 0).sum() * np.log(self.phi)

            # calculate the logliklihood
            log_lik = self._loglik(cur_y = cur_y, cur_z = cur_z, cur_r = cur_r)
        return log_prior + log_lik
            
    
class GibbsPredictor(BasePredictor):

    def __init__(self, cl_mode = True, cl_device = None,
                 alpha = 1.0, lam = 0.98, theta = 0.01, epislon = 0.02, init_k = 4):
        """Initialize the predictor.
        """
        BasePredictor.__init__(self, cl_mode = cl_mode, cl_device = cl_device)
        self.alpha = alpha
        self.lam = lam
        self.theta = theta
        self.epislon = epislon

    def read_test_csv(self, file_path, header=True):
        """Read the test cases and convert values to integer.
        """
        BasePredictor.read_test_csv(self, file_path, header)
        self.obs = np.array(self.obs, dtype=np.int32)
        return

    def read_samples_csv(self, var_name, file_path, header = True):
        """Read samples from a csv file.
        """
        BasePredictor.read_samples_csv(self, var_name, file_path, header)
        new_samples = []
        for sample in self.samples[var_name]:
            if len(sample) > 1: # remove null feature samples
                sample = np.array(sample, dtype=np.int32)
                sample = np.reshape(sample[1:], (-1, sample[0]))
                new_samples.append(sample)
        self.samples[var_name] = new_samples

    def predict(self, thining = 0, burnin = 0, use_iter=None, output_file = None):
        """Predict the test cases
        """
        assert('y' in self.samples and 'z' in self.samples)
        assert(len(self.samples['y']) == len(self.samples['z']))
        
        num_sample = len(self.samples['y'])
        num_obs = len(self.obs)
        logprob_result = np.empty((num_sample, num_obs))

        for i in xrange(num_sample):
            cur_y = self.samples['y'][i]
            cur_z = self.samples['z'][i]
            
            # generate all possible Zs
            num_feature = cur_z.shape[1]
            all_z = []
            for n in xrange(num_feature+1):
                base = [1] * n + [0] * (num_feature - n)
                all_z.extend(list(set(itertools.permutations(base))))
            all_z = np.array(all_z, dtype=np.int32)
            
            # BEGIN p(z|z_inferred) calculation

            # the following lines of code may be a bit tricky to parse
            # first, calculate the probability of features that already exist
            # since features are additive within an image, we can just prod them
            prior_off_prob = 1.0 - cur_z.sum(axis = 0) / float(cur_z.shape[0])
            prior_prob = np.abs(all_z - prior_off_prob)

            # then, locate the novel features in all_z
            mask = np.ones(all_z.shape)
            mask[:,np.where(cur_z.sum(axis = 0) > 0)] = 0
            novel_all_z = all_z * mask
            
            # temporarily mark those cells to have probability 1
            prior_prob[novel_all_z==1] = 1

            # we can safely do row product now, still ignoring new features
            prior_prob = prior_prob.prod(axis = 1)

            # let's count the number of new features for each row
            num_novel = novel_all_z.sum(axis = 1)
            # calculate the probability
            novel_prob = poisson.pmf(num_novel, self.alpha / float(cur_z.shape[0]))
            # ignore the novel == 0 special case
            novel_prob[num_novel==0] = 1.

            # multiply it by prior prob
            prior_prob = prior_prob * novel_prob
            
            # END p(z|z_inferred) calculation

            # BEGIN p(x|z, y_inferred)
            n_by_d = np.dot(all_z, cur_y)
            not_on_p = np.power(1. - self.lam, n_by_d) * (1. - self.epislon)
            for j in xrange(len(self.obs)):
                prob = np.abs(self.obs[j] - not_on_p).prod(axis=1) 
                prob = prob #* prior_prob
                prob = prob.sum()
                logprob_result[i,j] = prob
            # END
                
        return logprob_result.max(axis=0), logprob_result.std(axis=0)
