#-*- coding: utf-8 -*-

from __future__ import print_function
import sys, os.path
pkg_dir = os.path.dirname(os.path.realpath(__file__)) + '/../../'
sys.path.append(pkg_dir)

import pyopencl.array
from collections import Counter
from MPBNP import *

np.set_printoptions(suppress=True)

class CollapsedGibbs(BaseSampler):

    def __init__(self, cl_mode = True, inference_mode = True, alpha = 1.0, cl_device = None):
        """Initialize the class.
        """
        BaseSampler.__init__(self, cl_mode, inference_mode, cl_device)

        if cl_mode:
            program_str = open(pkg_dir + 'MPBNP/crp/kernels/crp_categorical_cl.c', 'r').read()
            self.prg = cl.Program(self.ctx, program_str).build()

        self.alpha = alpha 
        self.support = []
        self.support_size = []

    def read_csv(self, filepath, header=True):
        """Read the data from a csv file.
        """
        BaseSampler.read_csv(self, filepath, header)
        # get the discrete support for each dimension
        self.obs = np.array(self.obs)
        for i in xrange(self.obs.shape[1]):
            self.support.append(np.unique(self.obs[:,i]))
            self.support_size.append(len(self.support[i]))
        return

    def do_inference(self, init_labels = None, output_file = None):
        """Perform inference on the given observations assuming 
        data are generated by a Gaussian CRP Mixture Model.
        """
        BaseSampler.do_inference(self, output_file)

        # initialize cluster labels
        data_size = self.obs.shape[0]
        if init_labels is None:
            init_labels = np.random.randint(low = 0, high = min(data_size, 5), size = data_size)

        if self.cl_mode:
            return self.cl_infer_categorical(init_labels = init_labels, output_file = output_file)
        else:
            return self.infer_categorical(init_labels = init_labels, output_file = output_file)

    def generate(self, n = 1000, output_file = None):
        BaseSampler.generate(self, n, output_file)
        return

    def infer_categorical(self, init_labels, output_file = None):
        """Implementing concurrent sampling of partition labels without OpenCL.
        """
        try: dim = self.obs.shape[1]
        except IndexError: dim = 1
        
        data_size = self.obs.shape[0]
        cluster_labels = init_labels

        # set some prior hyperparameters
        beta = 0.1

        if output_file is not None: print(*xrange(data_size), file = output_file, sep = ',')

        # run
        for i in xrange(self.niter):
            if output_file is not None and i >= burnin: 
                print(*cluster_labels, file = output_file, sep = ',')            

            uniq_labels = np.unique(cluster_labels)
            _, _, new_cluster_label = smallest_unused_label(uniq_labels)
            uniq_labels = np.hstack((new_cluster_label, uniq_labels))
            num_of_clusters = uniq_labels.shape[0]

            # compute the sufficient statistics of each cluster
            n = {}
            counts = {}
            logpost = np.zeros((data_size, num_of_clusters))

            for label in uniq_labels:
                label_index = np.where(uniq_labels == label)[0]
                # set up the dictionary for storing counts
                counts[label] = {}
                if label == new_cluster_label:
                    n[label] = 0
                    # loop over all dimensions
                    for d in xrange(dim):
                        counts[label][d] = Counter()
                else:
                    cluster_obs = self.obs[np.where(cluster_labels == label)]
                    n[label] = cluster_obs.shape[0]
                    for d in xrange(dim):
                        counts[label][d] = Counter(cluster_obs[:,d])

                for o_index in xrange(len(self.obs)):
                    o = self.obs[o_index]
                    for d in xrange(dim):
                        logpost[o_index, label_index] += np.log((beta + counts[label][d][o[d]]) / (self.support_size[d] * beta + n[label]))

                logpost[:,label_index] += np.log(n[label]) if n[label] > 0 else np.log(self.alpha)

            # resample the labels and implement the changes
            for j in xrange(data_size):
                target_cluster = sample(a = uniq_labels, p = lognormalize(logpost[j]))
                cluster_labels[j] = target_cluster

        return Counter(cluster_labels).most_common()

    def cl_infer_categorical(self, init_labels, output_file = None):
        """Implementing concurrent sampling of class labels with OpenCL.
        """
        if not self.inference_mode: 
            print("Sorry. This function is only callable when the sampler is intialized in a inference mode")
            sys.exit(0)
        total_time = 0
        total_a_time = time()
        gpu_time = 0

        try: 
            dim = np.int32(self.obs.shape[1])
            if dim > 1:
                print("Sorry. The GPU sampler currently only supports 1-d categorical data.", file=sys.stderr)
                sys.exit(0)
        except IndexError: dim = np.int32(1)
        data_size = np.int32(self.obs.shape[0])

        # set some prior hyperparameters
        beta = np.float32(0.1)
        # set up cluster labels
        cluster_labels = init_labels.astype(np.int32)
        # get unique outcome types
        uniq_outcomes = np.unique(self.support[0])
        num_of_outcomes = np.int32(self.support_size[0])

        # push data and initial labels onto the openCL device
        # data won't change, labels are modified on the device
        # To make it easier to process in OpenCL C, data are converted to array indices according to uniq_outcomes
        d_data = cl.Buffer(self.ctx, self.mf.READ_ONLY | self.mf.COPY_HOST_PTR, 
                           hostbuf = np.where(uniq_outcomes == self.obs)[1].astype(np.int32))
        d_labels = cl.Buffer(self.ctx, self.mf.READ_WRITE | self.mf.COPY_HOST_PTR, hostbuf = cluster_labels)

        if output_file is not None: print(*xrange(data_size), file = output_file, sep = ',')

        
        for i in xrange(self.niter):
            if output_file is not None and i >= self.burnin: 
                print(*cluster_labels, file = output_file, sep = ',')            
            # at the beginning of each iteration, identity the unique cluster labels
            uniq_labels = np.unique(cluster_labels)
            _, _, new_cluster_label = smallest_unused_label(uniq_labels)
            uniq_labels = np.hstack((new_cluster_label, uniq_labels)).astype(np.int32)
            num_of_clusters = np.int32(uniq_labels.shape[0])

            # using OpenCL to compute the log posterior of each item and perform resampling
            a_time = time()

            # compute the sufficient statistics of each cluster
            d_count = cl.array.empty(self.queue, (uniq_labels.shape[0], uniq_outcomes.shape[0]), np.int32)
            d_n = cl.array.empty(self.queue, uniq_labels.shape, np.int32)
            d_uniq_label = cl.Buffer(self.ctx, self.mf.READ_ONLY | self.mf.COPY_HOST_PTR, hostbuf = uniq_labels)

            self.prg.compute_suff_stats(self.queue, (uniq_labels.shape[0],), None,
                                        d_uniq_label, d_labels, d_data, d_count.data, d_n.data,
                                        data_size, num_of_outcomes)

            d_rand = cl.Buffer(self.ctx, self.mf.READ_ONLY | self.mf.COPY_HOST_PTR, hostbuf = np.random.random(data_size).astype(np.float32))
            d_logpost = cl.array.empty(self.queue, (data_size, uniq_labels.shape[0]), np.float32)
            
            # if the OpenCL device is CPU, use the kernel with loops over clusters
            if self.device_type == cl.device_type.CPU:
                self.prg.cat_logpost_loopy(self.queue, (self.obs.shape[0],), None,
                                           d_labels, d_data, d_uniq_label, d_count.data, d_n.data,
                                           num_of_clusters, num_of_outcomes, np.float32(self.alpha),
                                           beta, d_logpost.data, d_rand)
            # otherwise, use the kernel that fully unrolls data points and clusters
            else:
                self.prg.cat_logpost(self.queue, (self.obs.shape[0], uniq_labels.shape[0]), None, 
                                     d_labels, d_data, d_uniq_label, d_count.data, d_n.data, 
                                     num_of_clusters, num_of_outcomes, np.float32(self.alpha),
                                     beta, d_logpost.data, d_rand)
                self.prg.resample_labels(self.queue, (self.obs.shape[0],), None,
                                         d_labels, d_uniq_label, num_of_clusters,
                                         d_rand, d_logpost.data)
                
            cl.enqueue_copy(self.queue, cluster_labels, d_labels)
            gpu_time += time() - a_time
            
        total_time += time() - total_a_time
        return gpu_time, total_time, Counter(cluster_labels).most_common()

if __name__ == '__main__':

    argv = sys.argv
    crp_sampler = CollapsedGibbs(cl_mode = True)
    crp_sampler.read_csv('../data/coin.csv')
    crp_sampler.set_sampling_params(niter = 1000, thining = 0, burnin = 0)

    gpu_time, total_time, most_common = crp_sampler.do_inference()
    print('Finished %d iterations\nOpenCL device time: %f seconds, Total time: %f seconds' % 
          (1000, gpu_time, total_time))
    
