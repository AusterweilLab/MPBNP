#!/usr/bin/env python2
#-*- coding: utf-8 -*-

from __future__ import print_function
import sys, os.path
pkg_dir = os.path.dirname(os.path.realpath(__file__)) + '/../../'
sys.path.append(pkg_dir)

import pyopencl.array
from collections import Counter
from scipy.stats import poisson
from MPBNP import *

np.set_printoptions(suppress=True)

class BiasedGibbs(BaseSampler):

    def __init__(self, cl_mode = True, inference_mode = True, cl_device = None,
                 alpha = 1.0, lam = 0.95, theta = 0.25, epislon = 0.05, init_k = 2):
        """Initialize the class.
        """
        BaseSampler.__init__(self, cl_mode, inference_mode, cl_device)

        if cl_mode:
            program_str = open(pkg_dir + './kernels/ibp_noisyor_cl.c', 'r').read()
            #utilities_str = open('kernels/utilities_cl.c', 'r').read()
            self.prg = cl.Program(self.ctx, program_str).build()
            #self.util = cl.Program(self.ctx, utilities_str).build()

        self.alpha = alpha # tendency to generate new features
        self.k = init_k    # initial number of features
        self.theta = theta # prior probability that a pixel is on in a feature image
        self.lam = lam # effecacy of a feature
        self.epislon = epislon # probability that a pixel is on by change in an actual image

    def read_csv(self, filepath, header=True):
        """Read the data from a csv file.
        """
        BaseSampler.read_csv(self, filepath, header)
        # convert the data to floats
        self.new_obs = []
        for row in self.obs:
            self.new_obs.append([int(_) for _ in row])
        self.obs = np.array(self.new_obs)
        self.d = len(self.obs[0])
        self.n = len(self.obs)
        return

    def direct_read_obs(self, obs):
        BaseSampler.read_csv(self, obs)
        self.d = len(self.obs[0])
        self.n = len(self.obs)
        
    def do_inference(self, 
                     init_y = None, init_z = None, init_f = None,
                     output_y_file = None, output_z_file = None, output_f_file = None):
        """Perform inference on the given observations assuming data are generated by an IBP model
        with noisy-or as the likelihood function.
        @param init_y: An initial list of two feature image matrices, where values are 0 or 1 for each matrix
        @param init_z: An initial feature ownership matrix, where values are 0 or 1
        @param init_f: An initial indexing matrix, where values are 0 or 1, and select which feature image matrix to use
        """
        BaseSampler.do_inference(self, output_file=None)
        if init_y is None:
            init_y = np.random.randint(0, 2, (2, self.k, self.d))
        else:
            assert(type(init_y) is np.ndarray)
            assert(init_y.shape == (self.k, self.d))
        if init_f is None:
            init_f = np.random.randint(0, 3, (self.n, self.k))
        else:
            assert(type(init_f) is np.ndarray)
            assert(init_f.shape == (self.n, self.k))
        if init_z is None:
            init_z = np.empty(init_f.shape, dtype=np.int32)
            init_z[np.where(init_f > 0)] = 1
            init_z[np.where(init_f == 0)] = 0
        else:
            assert(type(init_z) is np.ndarray)
            assert(init_z.shape == (self.n, self.k))
            
        if self.cl_mode:
            return self._cl_infer_yzf(init_y, init_z, init_f, output_y_file, output_z_file, output_f_file)
        else:
            return self._infer_yzf(init_y, init_z, init_f, output_y_file, output_z_file, output_f_file)

    def _infer_yzf(self, init_y, init_z, init_f, 
                   output_y_file = None, output_z_file = None, output_f_file = None):
        """Wrapper function to start the inference on y and z.
        
        WARNING: This method is not supposed to directly invoked by an end user.
                 Use do_inference() instead.

        @param init_y: Passed in from do_inference()
        @param init_z: Passed in from do_inference()
        @param init_f: Passed in from do_inference()
        """
        cur_y, cur_z, cur_f = init_y, init_z, init_f

        a_time = time()
        for i in xrange(self.niter):
            print('Y:', cur_y, sep='\n', file=sys.stderr)
            print('Z:', cur_z, sep='\n', file=sys.stderr)
            print('F:', cur_f, sep='\n', file=sys.stderr)
            print('DATA:', self.obs, sep='\n', file=sys.stderr)
            cur_y, cur_z, cur_f = self._infer_f(cur_y, cur_z, cur_f)
            cur_y = self._infer_y(cur_y, cur_z, cur_f)
            cur_y, cur_z, cur_f = self._infer_z(cur_y, cur_z, cur_f)

            raw_input()

            #self._sample_lam(cur_y, cur_z)
            if output_y_file is not None and i >= self.burnin: 
                print_matrix_in_row(cur_y, output_y_file)
            if output_z_file is not None and i >= self.burnin: 
                print_matrix_in_row(cur_z, output_z_file)


        print('Y:', cur_y, sep='\n', file=sys.stderr)
        print('Z:', cur_z, sep='\n', file=sys.stderr)
        print('F:', cur_f, sep='\n', file=sys.stderr)
        print('DATA:', self.obs, sep='\n', file=sys.stderr)

        return -1, time() - a_time, None

    def _infer_y(self, cur_y, cur_z, cur_f):
        """Infer feature image matrices Y
        """
        cur_zf = cur_z * cur_f
        # calculate the prior probability that a pixel is on
        y_on_log_prob = np.log(self.theta) * np.ones(cur_y.shape)
        y_off_log_prob = np.log(1. - self.theta) * np.ones(cur_y.shape)

        # calculate the likelihood
        on_loglik = np.empty(cur_y.shape)
        off_loglik = np.empty(cur_y.shape)
    
        for y_idx in xrange(cur_y.shape[0]):
            for row in xrange(cur_y[y_idx].shape[0]):
                affected_data_index = np.where(cur_zf[:,row] == y_idx+1)
                for col in xrange(cur_y[y_idx].shape[1]):
                    old_value = cur_y[y_idx, row, col]
                    cur_y[y_idx, row, col] = 1
                    on_loglik[y_idx, row, col] = self._loglik_nth(cur_y, cur_z, cur_f,
                                                                  n = affected_data_index)
                    cur_y[y_idx, row, col] = 0
                    off_loglik[y_idx, row, col] = self._loglik_nth(cur_y, cur_z, cur_f,
                                                                   n = affected_data_index)
                    cur_y[y_idx, row, col] = old_value

        # add to the prior
        y_on_log_prob += on_loglik
        y_off_log_prob += off_loglik

        ew_max = np.maximum(y_on_log_prob, y_off_log_prob)
        y_on_log_prob -= ew_max
        y_off_log_prob -= ew_max
        
        # normalize
        y_on_prob = np.exp(y_on_log_prob) / (np.exp(y_on_log_prob) + np.exp(y_off_log_prob))
        cur_y = np.random.binomial(1, y_on_prob)

        return cur_y

    def _infer_f(self, cur_y, cur_z, cur_f, f_prior=None):
        """Infer feature ownership matrix Z.
        """
        # add loglikelihood of data
        for row in xrange(self.n):
            z_col_sum = cur_z.sum(axis = 0)

            # calculate the IBP prior on feature ownership for existing features
            m_minus = z_col_sum - cur_z
            active_prob = m_minus / float(self.n)
            inactive_prob = 1 - m_minus / float(self.n)
            
            #print(cur_y, cur_z, cur_f, sep='\n-----\n')

            for col in xrange(cur_f.shape[1]):
                #print('row:', row, 'col:', col)
                if f_prior is None:
                    f_prior = [((cur_f == 1).sum() - int(cur_f[row,col] == 1) + .1) / ((cur_f > 0).sum() - int(cur_f[row,col] > 0) + .2),
                               ((cur_f == 2).sum() - int(cur_f[row,col] == 2) + .1) / ((cur_f > 0).sum() - int(cur_f[row,col] > 0) + .2)]

                prob_grid = np.array([inactive_prob[row, col], 
                                      active_prob[row, col] * f_prior[0], 
                                      active_prob[row, col] * f_prior[1]]) # a uniform prior on choosing either 1 or 2 in F

                #print('priors:', prob_grid)
                lik_grid = np.empty(3)
                for i in xrange(3):
                    cur_f[row, col] = i
                    cur_z[row, col] = int(i > 0)
                    lik_grid[i] = np.exp(self._loglik_nth(cur_y, cur_z, cur_f, n = row))
                    prob_grid[i] = prob_grid[i] * lik_grid[i]
                    #print(i, np.exp(self._loglik_nth(cur_y, cur_z, cur_f, n = row)), file=sys.stderr)


                # normalize the probability
                prob_grid = prob_grid / prob_grid.sum()
                #print('likelihoods:', lik_grid)
                #print('posteriors:', prob_grid)
                #raw_input()
                cur_f[row, col] = np.random.choice(a = 3, p = prob_grid)
                # set z accordingly
                cur_z[row, col] = int(cur_f[row, col] > 0)

            # sample new features
            cur_y, cur_z, cur_f = self._sample_k_new(cur_y, cur_z, cur_f, row)

            #raw_input()
        return cur_y, cur_z, cur_f

    def _infer_z(self, cur_y, cur_z, cur_f):
        """Sample new features use MH.

        Note: In this particular model, performing inference on existing features
        p(Z | F, Y, X) is unnecessary, as the inference of p(F | Y, Z, X) already
        takes care of the inference of Z. See _infer_f() for why this is the case.

        Therefore, _infer_z() is devoted to the sampling of new features.
        """
        # delete null features
        inactive_feat_col = np.where(cur_z.sum(axis = 0) == 0)
        cur_z = np.delete(cur_z, inactive_feat_col[0], axis=1)
        cur_f = np.delete(cur_f, inactive_feat_col[0], axis=1)
        cur_y = np.array([np.delete(cur_y[0], inactive_feat_col[0], axis=0),
                          np.delete(cur_y[1], inactive_feat_col[0], axis=0)])

        # update self.k
        self.k = cur_z.shape[1]
        
        return cur_y, cur_z, cur_f

    def _sample_k_new(self, cur_y, cur_z, cur_f, n):
        """Sample new features for the nth row of F (and Z)
        """
        k_new_count = np.random.poisson(self.alpha / self.n)
        if k_new_count == 0: return cur_y, cur_z, cur_f

        # calculate the old logliklihood
        old_loglik = self._loglik_nth(cur_y, cur_z, cur_f, n)
            
        f_prior = [((cur_f == 1).sum() + .01) / ((cur_f > 0).sum() + .02),
                   ((cur_f == 2).sum() + .01) / ((cur_f > 0).sum() + .02)]

        # create the new F vector
        new_f = np.zeros((self.n, k_new_count), dtype = np.int32)
        new_f[n, :k_new_count] = np.random.choice(a = [1, 2], p = f_prior, size = k_new_count)
        cur_f_new = np.hstack((cur_f, new_f))
        # create the new Z vector
        cur_z_new = np.empty(cur_f_new.shape, dtype=np.int32)
        cur_z_new[np.where(cur_f_new > 0)] = 1
        cur_z_new[np.where(cur_f_new == 0)] = 0
        # propose feature images by sampling from the prior distribution
        cur_y_new = np.array([
                np.vstack((cur_y[0], np.random.binomial(1, self.theta, (k_new_count, self.d)))),
                np.vstack((cur_y[1], np.random.binomial(1, self.theta, (k_new_count, self.d))))
                ])
    
        new_loglik = self._loglik_nth(cur_y_new, cur_z_new, cur_f_new, n)

        # normalization
        max_loglik = max(new_loglik, old_loglik)
        new_loglik -= max_loglik
        old_loglik -= max_loglik
        # sampling
        move_prob = 1 / (1 + np.exp(old_loglik - new_loglik))
        if random.random() < move_prob:
            cur_y = cur_y_new
            cur_z = cur_z_new
            cur_f = cur_f_new

        return cur_y, cur_z, cur_f
        
    def _sample_lam(self, cur_y, cur_z):
        """Resample the value of lambda.
        """
        old_loglik = self._loglik(cur_y, cur_z)
        old_lam = self.lam
    
        # modify the feature ownership matrix
        self.lam = np.random.beta(1,1)
        new_loglik = self._loglik(cur_y, cur_z)
        move_prob = 1 / (1 + np.exp(old_loglik - new_loglik));
        if random.random() < move_prob:
            pass
        else:
            self.lam = old_lam

    def _sample_epislon(self, cur_y, cur_z):
        """Resample the value of epislon
        """
        old_loglik = self._loglik(cur_y, cur_z)
        old_epislon = self.epislon
    
        # modify the feature ownership matrix
        self.epislon = np.random.beta(1,1)
        new_loglik = self._loglik(cur_y, cur_z)
        move_prob = 1 / (1 + np.exp(old_loglik - new_loglik));
        if random.random() < move_prob:
            pass
        else:
            self.epislon = old_epislon

    def _loglik_nth(self, cur_y, cur_z, cur_f, n):
        """Calculate the loglikelihood of the nth data point
        given Y and Z.
        """
        assert(cur_y.shape[0] == 2 and cur_z.shape[1] == cur_y.shape[1])

        loglik = 0

        if type(n) is int: n = ([n],)

        for each_n in n[0]:
            y_comb = np.zeros(cur_y[0].shape, dtype=np.int32)
            y_comb[np.where(cur_f[each_n] == 1)] = cur_y[0][np.where(cur_f[each_n] == 1)]
            y_comb[np.where(cur_f[each_n] == 2)] = cur_y[1][np.where(cur_f[each_n] == 2)]
            
            not_on_p = np.power(1. - self.lam, np.dot(cur_z[each_n], y_comb)) * (1. - self.epislon)

            loglik += np.log(np.abs(self.obs[each_n] - not_on_p)).sum()

        return loglik

    def _loglik(self, cur_y, cur_z, cur_f):
        """Calculate the loglikelihood of data given Y and Z.
        """
        assert(cur_y.shape[0] == 2 and cur_z.shape[1] == cur_y.shape[1])

        n_by_d = np.empty((cur_z.shape[0], cur_y.shape[2]), dtype=np.int32)
        for row in xrange(cur_z.shape[0]):
            y_comb = np.zeros(cur_y[0].shape, dtype=np.int32)
            y_comb[np.where(cur_f[row] == 1)] = cur_y[0][np.where(cur_f[row] == 1)]
            y_comb[np.where(cur_f[row] == 2)] = cur_y[1][np.where(cur_f[row] == 2)]
            n_by_d[row] = np.dot(cur_z[row], y_comb)

        not_on_p = np.power(1. - self.lam, n_by_d) * (1. - self.epislon)
        loglik_mat = np.log(np.abs(self.obs - not_on_p))

        return loglik_mat.sum()

    def _cl_infer_yz(self, init_y, init_z, output_y_file = None, output_z_file = None):
        """Wrapper function to start the inference on y and z.
        This function is not supposed to directly invoked by an end user.
        @param init_y: Passed in from do_inference()
        @param init_z: Passed in from do_inference()
        """
        cur_y = init_y.astype(np.int32)
        cur_z = init_z.astype(np.int32)
        d_obs = cl.Buffer(self.ctx, self.mf.READ_ONLY | self.mf.COPY_HOST_PTR, hostbuf=self.obs.astype(np.int32))

        gpu_time = 0
        total_time = 0
        for i in xrange(self.niter):
            a_time = time()
            cur_y = self._cl_infer_y(cur_y, cur_z, d_obs)
            cur_z = self._cl_infer_z(cur_y, cur_z, d_obs)
            gpu_time += time() - a_time
            cur_y, cur_z = self._cl_infer_k_new(cur_y, cur_z)
            if output_y_file is not None and i >= self.burnin: 
                print_matrix_in_row(cur_y, output_y_file)
            if output_z_file is not None and i >= self.burnin: 
                print_matrix_in_row(cur_z, output_z_file)
            total_time += time() - a_time

        return gpu_time, total_time, None

    def _cl_infer_y(self, cur_y, cur_z, d_obs):
        """Infer feature images
        """
        d_cur_y = cl.Buffer(self.ctx, self.mf.READ_WRITE | self.mf.COPY_HOST_PTR, hostbuf = cur_y.astype(np.int32))
        d_cur_z = cl.Buffer(self.ctx, self.mf.READ_WRITE | self.mf.COPY_HOST_PTR, hostbuf = cur_z.astype(np.int32))
        d_z_by_y = cl.Buffer(self.ctx, self.mf.READ_ONLY | self.mf.COPY_HOST_PTR, 
                             hostbuf = np.dot(cur_z, cur_y).astype(np.int32))
        d_rand = cl.Buffer(self.ctx, self.mf.READ_ONLY | self.mf.COPY_HOST_PTR, 
                           hostbuf=np.random.random(size = cur_y.shape).astype(np.float32))

        # calculate the prior probability that a pixel is on
        self.prg.sample_y(self.queue, cur_y.shape, None,
                          d_cur_y, d_cur_z, d_z_by_y, d_obs,
                          d_rand, #d_y_on_loglik.data, d_y_off_loglik.data,
                          np.int32(self.obs.shape[0]), np.int32(self.obs.shape[1]), np.int32(cur_y.shape[0]),
                          np.float32(self.lam), np.float32(self.epislon), np.float32(self.theta))

        cl.enqueue_copy(self.queue, cur_y, d_cur_y)
        return cur_y

    def _cl_infer_z(self, cur_y, cur_z, d_obs):
        """Infer feature ownership
        """
        d_cur_y = cl.Buffer(self.ctx, self.mf.READ_WRITE | self.mf.COPY_HOST_PTR, hostbuf = cur_y.astype(np.int32))
        d_cur_z = cl.Buffer(self.ctx, self.mf.READ_WRITE | self.mf.COPY_HOST_PTR, hostbuf = cur_z.astype(np.int32))
        d_z_by_y = cl.Buffer(self.ctx, self.mf.READ_ONLY | self.mf.COPY_HOST_PTR, 
                             hostbuf = np.dot(cur_z, cur_y).astype(np.int32))
        d_z_col_sum = cl.Buffer(self.ctx, self.mf.READ_WRITE | self.mf.COPY_HOST_PTR, 
                                hostbuf = cur_z.sum(axis = 0).astype(np.int32))
        d_rand = cl.Buffer(self.ctx, self.mf.READ_ONLY | self.mf.COPY_HOST_PTR, 
                           hostbuf=np.random.random(size = cur_z.shape).astype(np.float32))

        # calculate the prior probability that a pixel is on
        self.prg.sample_z(self.queue, cur_z.shape, None,
                          d_cur_y, d_cur_z, d_z_by_y, d_z_col_sum, d_obs,
                          d_rand, #d_z_on_loglik.data, d_z_off_loglik.data,
                          np.int32(self.obs.shape[0]), np.int32(self.obs.shape[1]), np.int32(cur_z.shape[1]),
                          np.float32(self.lam), np.float32(self.epislon), np.float32(self.theta))

        cl.enqueue_copy(self.queue, cur_z, d_cur_z)
        return cur_z
        
    def _cl_infer_k_new(self, cur_y, cur_z):

        # sample new features use importance sampling
        k_new = self._sample_k_new(cur_y, cur_z)
        if k_new:
            cur_y, cur_z = k_new

        # delete null features
        inactive_feat_col = np.where(cur_z.sum(axis = 0) == 0)
        cur_z_new = np.delete(cur_z, inactive_feat_col[0], axis=1).astype(np.int32)
        cur_y_new = np.delete(cur_y, inactive_feat_col[0], axis=0).astype(np.int32)

        z_new_s0, z_new_s1 = cur_z_new.shape
        cur_z_new = cur_z_new.reshape((z_new_s0 * z_new_s1, 1))
        cur_z_new = cur_z_new.reshape((z_new_s0, z_new_s1))

        y_new_s0, y_new_s1 = cur_y_new.shape
        cur_y_new = cur_y_new.reshape((y_new_s0 * y_new_s1, 1))
        cur_y_new = cur_y_new.reshape((y_new_s0, y_new_s1))

        # update self.k
        self.k = cur_z_new.shape[1]
        
        return cur_y_new, cur_z_new

class UniformGibbs(BiasedGibbs):

    def _infer_f(self, cur_y, cur_z, cur_f, f_prior=None):
        """Infer feature ownership matrix Z.
        """
        return super(UniformGibbs, self)._infer_f(cur_y, cur_z, cur_f, [0.5, 0.5])    

if __name__ == '__main__':

    NITER = 200
    #ibp_sampler = IBPNoisyOrTwoYUniformGibbs(cl_mode = False)
    ibp_sampler = BiasedGibbs(cl_mode = False)
    ibp_sampler.read_csv(pkg_dir + 'MPBNP/data/ibp-image-n8.csv')
    ibp_sampler.set_sampling_params(niter = NITER)
    ibp_sampler.do_inference()
